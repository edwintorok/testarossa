#!/usr/bin/env ocamlscript
Ocaml.packs := ["lwt"; "xen-api-client.lwt"; "cmdliner"; "re"; "ezxmlm"];
Ocaml.sources := ["../scripts/yorick.ml"; "test_common.ml"]
--

open Yorick
open Lwt
open Xen_api
open Xen_api_lwt_unix
open Test_common

let n = 16 (* TODO: match vagrantfile *)

let snapshot_name = "testarossa_clean"

let update_box () =
  echo "Updating vagrant box to latest version";
  update_box "infrastructure";
  update_box "cluster1"

type reset_mode = Force | Snapshot
let reset_vms reset_mode =
  (* TODO: determine current state, check whether we should start/stop/etc.
     whether snapshot already exists, etc *)
    begin if reset_mode = Snapshot then
      let new_name = Printf.sprintf "before reset %f" (Unix.gettimeofday ()) in
      (* start wouldn't be needed, but vagrant needs to know about the VM, it must be created at least *)
      echo "Starting all VMs";
      start_all "cluster" n;
      echo "Snapshotting VMs before reset";
      snapshot_all "cluster" n ~new_name
    else
      Lwt.return []
  end >>= fun _ ->
    echo "Reseting all cluster & infrastructure VMs to snapshot";
    (* TODO: put unique id that identifies this cluster with this many nodes, timestamp etc.
       so we are sure we restore the right snapshot

    *)
    revert_all "cluster" n ~snapshot_name >>= fun () ->
    start_all "cluster" n;
    Lwt.return_unit

let prepare () =
  echo "Starting up cluster";
  (* TODO: infrastructure needs to be started before cluster,
     or remove provisioninig dependency *)
  initialize_all "cluster" n;
  echo "Snapshotting all VMs";
  snapshot_all "cluster" n ~consistent:true ~new_name:snapshot_name >>= fun _ ->
  Lwt.return_unit

let sequential_join_remove () =
  echo "Setting up cluster on 1st node";
  run_script ~host:"cluster1" ~script:"setup_cluster_one.sh";

  echo "Joining nodes sequentially";
  for i = 2 to n do
    let node = Printf.sprintf "cluster%d" i in
    run_script ~host:node ~script:"destroy_cluster.sh";
    run_script ~host:"cluster1" ~script:("join_cluster.sh " ^ node);
  done
  (* TODO: when taking clean snapshots make sure they are all shut down
     and take aa consistent snapshot of the whole cluster then *) 

 (* TODO: sanity checks pcs cluster status consistent, etc.
  had iptables problems where online/offline was not consistent
 *)

  (* TODO: assert all nodes are up and didn't fence *)

  (*  TODO: in parallel *)

  (* might be useful to snapshot here!
     or have automatic snapshotting based on states we want to reach
  *)
(*
  echo "Removing nodes sequentially";
  let last = Printf.sprintf "cluster%d" n in
  for i = 1 to n-1 do
    let node = Printf.sprintf "cluster%d" i in
    run_script ~host:last ~script:("remove_node.sh " ^ node);
  done*)

let run_test () =
  sequential_join_remove ();
  Lwt.return_unit
(*    echo "Running quicktest...";
    match !?* (?|>) "%s" quicktest_cmd with
    | (_, 0) ->
      echo "Quicktest finished successfully!";
      Lwt.return ();
    | (stdout, rc) ->
      echo "---[ BEGIN OUTPUT FROM QUICKTEST ]---";
      echo "%s" (trim stdout);
      echo "---[  END OUTPUT FROM QUICKTEST  ]---";
      echo "Quicktest failed (exit code %d)" rc;
      exit rc;
*)

type mode = Skip | Reset of reset_mode | Recreate | Destroy | Provision

let main update mode =
  kill_children_at_exit ();
  if update then
    update_box ();
  run_and_self_destruct (begin match mode with
  | Some (Reset reset_mode) ->
    reset_vms reset_mode
  | Some Recreate ->
    echo "Destroying cluster";
    destroy_all "cluster" n;
    prepare ()
  | Some Destroy ->
    echo "Destroying cluster";
    destroy_all "cluster" n;
    Lwt.return_unit
  | Some Provision ->
    provision_all "cluster" n;
    Lwt.return_unit
  | None ->
    prepare ()
  | Some Skip ->  Lwt.return_unit
  end >>= fun () ->
  run_test ())

open Cmdliner

let mode_arg =
  let modes = [
    ("skip-prepare", Skip),
    "assumes the test environment is already prepared (VMs running),\
     and skips straight to executing the tests.";

    ("reset", Reset Snapshot),
    "takes a snapshot of the cluster (to avoid loosing logs needed for debugging),
     and resets all the cluster VMs to the first clean snapshot.";

    ("force-reset", Reset Force),
    "resets all the cluster VMs to the first clean snapshot, \
     DELETING all the changes made inside the VMs.";

    ("recreate", Recreate),
    "destroy all vagrant cluster VMs (note: doesn't delete snapshots yet),\
    recreate and reprovision them";

    ("provision", Provision),
    "rerun (ansible) provisioning on cluster nodes.";

    ("destroy", Destroy),
    "destroy all the cluster&infrastructure VMs."
  ] in
  let docv = "MODE" in
  let doc =
    Printf.sprintf "$(docv) must be %s. %s"
      (Arg.doc_alts_enum (List.map fst modes))
      (modes |> List.map (fun ((mode, _), mode_doc) ->
           Printf.sprintf "%s mode %s" (Arg.doc_quote mode) mode_doc) |>
      String.concat ". ")
  in
  Arg.(value & opt (modes |> List.map fst |> enum |> some) None & info ["mode"] ~docv ~doc)
  
let update_arg =
  let doc = "Update Vagrant boxes (pull new XVA builds).
             By default we do not update the templates to make dev/debug easier.
             Automated builds should always enable --update though."
  in
  Arg.(value & flag & info ~doc ["update"])

let main_t = Term.(pure main $ update_arg $ mode_arg)

let info =
  let doc = "Run pcs cluster test" in
  let man = [ `S "BUGS"; `P "Report bug on the github issue tracker" ] in
  Term.info "test_cluster" ~version:"0.1" ~doc ~man

let () =
  Term.exit @@ Term.eval (main_t, info)
